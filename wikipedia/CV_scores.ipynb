{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CV_scores.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RX0cDmVpOpBy"},"source":["# Evaluating Cross-Validation Trees\n","Files created in the notebook **create_CV_CategoryTrres.ipynb** were previously uploaded to Google Cloud Storage and made public\n","\n","# Choose Fold to evaluate\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"czvfYHcDNxLm","colab":{}},"source":["run_num = 5 # Any integer from 1 to 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2zu62MvRPJk1"},"source":["# Loading files from Google Cloud Storage\n","Downloading training and validation trees and a json file containing credentials to upload the scoring results back to GCS\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"paYWwPaGNv9K","outputId":"e2eea63a-52c4-438b-8e8c-f95e8431c8d8","colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["url = 'https://storage.googleapis.com/capstone_wikipedia/'\n","\n","train_file = 'tree_train_{}.pkl'.format(run_num)\n","train_url = url + train_file\n","\n","val_file = 'tree_val_{}.pkl'.format(run_num)\n","val_url = url + val_file\n","\n","!wget $train_url\n","!wget $val_url\n","!wget https://storage.googleapis.com/capstone_wikipedia/crack-petal-273320-bdcfa7d69d7a.json"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-05 22:41:56--  https://storage.googleapis.com/capstone_wikipedia/tree_train_5.pkl\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.12.208\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.12.208|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3206056759 (3.0G) [application/octet-stream]\n","Saving to: ‘tree_train_5.pkl’\n","\n","tree_train_5.pkl    100%[===================>]   2.99G  5.75MB/s    in 7m 21s  \n","\n","2020-05-05 22:49:17 (6.93 MB/s) - ‘tree_train_5.pkl’ saved [3206056759/3206056759]\n","\n","--2020-05-05 22:49:18--  https://storage.googleapis.com/capstone_wikipedia/tree_val_5.pkl\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.10.240\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.10.240|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 751170264 (716M) [application/octet-stream]\n","Saving to: ‘tree_val_5.pkl’\n","\n","tree_val_5.pkl      100%[===================>] 716.37M  5.62MB/s    in 3m 29s  \n","\n","2020-05-05 22:52:47 (3.43 MB/s) - ‘tree_val_5.pkl’ saved [751170264/751170264]\n","\n","--2020-05-05 22:52:47--  https://storage.googleapis.com/capstone_wikipedia/crack-petal-273320-bdcfa7d69d7a.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.10.16\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.10.16|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2323 (2.3K) [application/json]\n","Saving to: ‘crack-petal-273320-bdcfa7d69d7a.json.1’\n","\n","crack-petal-273320- 100%[===================>]   2.27K  --.-KB/s    in 0.02s   \n","\n","2020-05-05 22:52:49 (144 KB/s) - ‘crack-petal-273320-bdcfa7d69d7a.json.1’ saved [2323/2323]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9iwtlUCCXFgo"},"source":["# Import packages"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pVZfY4k4KcNJ","outputId":"9bcbb779-b85a-4221-b116-49c5eb8dd711","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import os\n","import json\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.corpus import stopwords\n","import numpy as np \n","\n","import nltk\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_IsRbzqBXLiV"},"source":["## Setting up GCS credentials\n","\n","### Helper functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F8DE9VtgxoAT","colab":{}},"source":["def get_bucket(client, bucket):\n","    return storage_client.get_bucket(bucket)\n","\n","def download_file(bucket, file):\n","    blob = bucket.blob(file)\n","    blob.download_to_filename(file)\n","\n","def upload_file(bucket, file):\n","    blob = bucket.blob(file)\n","    blob.upload_from_filename(file)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fsvUdHdFxD8F","colab_type":"text"},"source":["### Credentials"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1obUuC5_w0FA","colab":{}},"source":["os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"crack-petal-273320-bdcfa7d69d7a.json\"\n","storage_client = storage.Client()\n","bucket = get_bucket(storage_client, 'capstone_wikipedia')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-jh9HDCuXekD"},"source":["## Ad-hoc version of tree. Optimized for validation speed\n","To speed up the process, the trees created in **create_CV_CategoryTrres.ipynb** are pickled dictionaries ready to be loaded. The **load** method reads and unpickles them"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WbtdFyQTKVu1","colab":{}},"source":["MAX_FEATURES = 75000\n","\n","class CategoryNode:\n","    def __init__(self, category):\n","        self.category = category\n","        self.children = []\n","        self.pages = []\n","        self.texts = []\n","        self.vectors = []\n","        \n","    def add_child(self, node):\n","        self.children.append(node)\n","                \n","    def __str__(self):\n","        return self.category\n","    \n","    def delete_child(self, cat):\n","        del_index = None\n","        for i, child in enumerate(self.children):\n","            if child.category == cat:\n","                del_index = i \n","                break\n","        del self.children[del_index]\n","        \n","    def infer_vector(self):\n","        vectors = []\n","        for child in self.children:\n","            try:\n","                if np.all((child.vectors != 0)):\n","                    if type(vectors) is list:\n","                        vectors = child.vectors\n","                    else:\n","                        vectors = np.append(vectors, child.vectors, axis = 0)\n","            except ValueError:\n","                continue\n","                \n","        if type(vectors) is not list:        \n","            self.vectors = np.mean(vectors, axis = 0).reshape((1, MAX_FEATURES))\n","        else:\n","            self.vectors = np.zeros((1, MAX_FEATURES))\n","\n","class CategoryTree:\n","    def __init__(self, vectorizer):\n","        self.root = CategoryNode(\"Root\")\n","        self.vectorizer = vectorizer\n","        \n","    def load(self, file):\n","        with open(file, 'rb') as f:\n","          tree = pickle.load(f)\n","          \n","        corpus = []\n","        print(\"Category Tree loading...\")\n","        \n","        unchecked_nodes = [(name, self.root, []) for name in tree.keys()]\n","        \n","        while len(unchecked_nodes) > 0:\n","            cat, parent_node, path = unchecked_nodes[0]\n","            unchecked_nodes = unchecked_nodes[1:]\n","\n","            node = CategoryNode(cat)\n","            parent_node.add_child(node)\n","\n","            section = tree\n","            for p in path:\n","                section = section[p]\n","\n","            node.pages = section[cat]['pages']\n","            node.texts = section[cat]['texts']\n","            corpus += node.texts\n","            cat_children = set(section[cat]) - set([\"pages\",\"texts\"])\n","\n","            node_children = [(child, node, path + [cat]) for child in cat_children]\n","            unchecked_nodes += node_children\n","        \n","        \n","        print(\"Vectorizer is fitting...\")\n","        self.vectorizer.fit(corpus)\n","        \n","        del corpus\n","        del tree\n","        \n","        print(\"Transforming tree...\")\n","        self.vectorize()\n","        \n","        print(\"Tree is ready!\")\n","        \n","    def load_vectorizer(self, vectorizer_filename, pages_filename):\n","        self.vectorizer = pickle.load(open(vectorizer_filename, 'rb'))\n","\n","        tree = json.load(open(pages_filename,'r'))\n","        tree = json.loads(tree)\n","        print(\"Category Tree loading...\")\n","        \n","        unchecked_nodes = [(name, self.root, []) for name in tree.keys()]\n","        \n","        while len(unchecked_nodes) > 0:\n","            cat, parent_node, path = unchecked_nodes[0]\n","            unchecked_nodes = unchecked_nodes[1:]\n","\n","            node = CategoryNode(cat)\n","            parent_node.add_child(node)\n","\n","            section = tree\n","            for p in path:\n","                section = section[p]\n","\n","            node.pages = section[cat]['pages']\n","            node.texts = section[cat]['texts']\n","            cat_children = set(section[cat]) - set([\"pages\",\"texts\"])\n","\n","            node_children = [(child, node, path + [cat]) for child in cat_children]\n","            unchecked_nodes += node_children\n","        \n","        del tree\n","        \n","        print(\"Transforming tree...\")\n","        self.vectorize()\n","        \n","        print(\"Tree is ready!\")\n","    \n","    def save_vectorizer(self, vectorizer_filename):\n","        pickle.dump(self.vectorizer, open(vectorizer_filename, 'wb'))\n","        \n","    def vectorize(self):      \n","        bad_nodes = []\n","        unchecked_nodes = [child for child in self.root.children]\n","        while len(unchecked_nodes) != 0:\n","            node = unchecked_nodes[0]\n","            unchecked_nodes = unchecked_nodes[1:]\n","            try:\n","                node.vectors = self.vectorizer.transform(node.texts)\n","            except ValueError:\n","                node.vectors = np.zeros((1, MAX_FEATURES))\n","                bad_nodes.append(node)\n","                \n","            unchecked_nodes += node.children\n","\n","        for node in bad_nodes:\n","            node.infer_vector()\n","        \n","                \n","    def search(self, words, similarity_metric, fingerprint=False, depth=2):\n","        if not fingerprint:\n","            input_vector = self.vectorizer.transform(words)\n","        else:\n","            input_vector = np.array([0 if _ not in words.keys() else 1 for idx,_ in enumerate(self.vectorizer.get_feature_names())]).reshape([1,len(self.vectorizer.get_feature_names())])\n","        result = []\n","        current_node = self.root\n","\n","        cnt = 1\n","        while len(current_node.children) != 0:\n","            cat_maximum = 0, None\n","            page_maximum = 0, None\n","            \n","            for node in current_node.children:\n","                cat_vec = np.mean(node.vectors, axis=0)\n","                if np.max(cat_vec) == 0:\n","                    continue\n","                test = similarity_metric(cat_vec, input_vector)[0][0]\n","                if test > cat_maximum[0]:\n","                    cat_maximum = test, node\n","                    \n","            for i, vec in enumerate(cat_maximum[1].vectors):\n","                test = similarity_metric(vec, input_vector)[0][0]\n","                if test > page_maximum[0]:\n","                    if len(cat_maximum[1].pages) != 0:\n","                        page_maximum = test, cat_maximum[1].pages[i]\n","                    else:\n","                        page_maximum = None\n","                        \n","            result.append(cat_maximum)\n","            result.append(page_maximum)\n","            \n","            current_node = cat_maximum[1]\n","            \n","            cnt += 1\n","            if cnt>depth:\n","              break\n","        \n","        return result"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xn80g8WdX7qq"},"source":["## Training tree and backing up vectorizer\n","\n","### Helper functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pCjr9npWxDzM","colab":{}},"source":["def score_text(txt, category_lst, tree, metric):\n","  '''\n","      Scores ONE text and returns a binary list representing True Positives as 1,\n","      and Otherwise 0\n","\n","      INPUT:\n","        txt: string with the text to be evaluated\n","        category_lst: list of ground truth for Categories and Subcategories\n","        tree: an instance of CategoryTree previously trained\n","        metric: metric to evaluate similarity between groun truth and predictions.\n","                Usually cosine similarity\n","      OUTPUT:\n","        Binary list of the type [cat1, cat2, cat3] where 1 means a True Positive and \n","        0 means an incorrect prediction\n","  '''\n","  predicted = [(x[0], str(x[1])) for x in tree.search([txt], metric) if type(x[1]) is not str]\n","\n","  classified = np.zeros(3)\n","  for i, cats in enumerate(zip(category_lst, predicted)):\n","    if cats[0]==cats[1][1]:\n","      classified[i] = 1\n","  return list(classified)\n","\n","\n","def score_texts(texts, category_lst, tree, metric):\n","  '''\n","      Scores a list of texts and returns a list of binary lists representing \n","      True Positives as 1, and Otherwise 0\n","\n","      INPUT:\n","        txt: list of string with the texts to be evaluated\n","        category_lst: list of ground truth for Categories and Subcategories\n","        tree: an instance of CategoryTree previously trained\n","        metric: metric to evaluate similarity between groun truth and predictions.\n","                Usually cosine similarity\n","      OUTPUT:\n","        List of Binary lists of the type [cat1, cat2, cat3] where 1 means a \n","        True Positive and 0 means an incorrect prediction\n","  '''\n","  scores_lst = []\n","\n","  for txt in texts:\n","    scores_lst.append(category_lst + score_text(txt, category_lst, tree, metric))\n","  \n","  return scores_lst\n","\n","def score_text_skip_exceptions(texts, category_lst, tree, metric):\n","  '''This is a function to work around situations like evaluating an empty text\n","  or evaluating a text using a category not contained in the trained model'''\n","  scores_lst = []\n","\n","  for txt in texts:\n","    try:\n","      scores_lst.append(category_lst + score_text(txt, category_lst, tree, metric))\n","    except:\n","      pass\n","  \n","  return scores_lst"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"62v8AdKK3o4O","colab_type":"text"},"source":["### Train tree and upload vecotrizer to GCS"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iMX2oQdNWaKD","outputId":"c5b30437-2883-4063-90e8-87f2ea5642fc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tree = CategoryTree(TfidfVectorizer(stop_words=list(stopwords.words('english')),\n","                                        max_features=MAX_FEATURES))\n","tree.load(train_file)\n","\n","vectorizer_file = 'vectorizer_train_{}.pkl'.format(run_num)\n","tree.save_vectorizer(vectorizer_file)\n","upload_file(bucket, vectorizer_file)\n","print('Vectorizer upload successful')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Vectorizer upload successful\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DlvMpaFs3Yoc","colab_type":"text"},"source":["### Evaluate Validation set (tree)"]},{"cell_type":"code","metadata":{"id":"8HGaX3a43aND","colab_type":"code","colab":{}},"source":["with open(val_file, 'rb') as f:\n","  val = pickle.load(f)\n","print('Validation tree loaded')\n","\n","scores_lst = []\n","\n","for k1, v1 in val.items():\n","  category_lst = [k1, None, None]\n","  print(k1)\n","  scores_lst += score_text_skip_exceptions(v1['texts'], category_lst, tree, cosine_similarity)\n","  for k2, v2 in v1.items():\n","    if k2 in ['texts','pages']:\n","      continue\n","    category_lst = [k1, k2, None]\n","    print('\\t', k2)\n","    scores_lst += score_text_skip_exceptions(v2['texts'], category_lst, tree, cosine_similarity)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9jIMc2sP3a6h","colab_type":"text"},"source":["### Export results and exports them to GCS"]},{"cell_type":"code","metadata":{"id":"hsz_q2d73bYt","colab_type":"code","colab":{}},"source":["output_file = 'scores_cv_{}.csv'.format(run_num)\n","pd.DataFrame(scores_lst).to_csv(output_file, index=False)\n","upload_file(bucket, output_file)"],"execution_count":0,"outputs":[]}]}